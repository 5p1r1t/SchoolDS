{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ_10: Нейронные сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1: Написать на PyTorch forward и backward полносвязного слоя без использования autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1. / (1 + torch.exp(-x))\n",
    "\n",
    "def sigmoid_backward(da, x):\n",
    "    sig = sigmoid(x)\n",
    "\n",
    "    return da * sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return torch.maximum(torch.zeros(1), x)\n",
    "\n",
    "def relu_backward(da, x):\n",
    "    da = torch.tensor(da)\n",
    "    da[x <= 0] = 0\n",
    "    return da"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "outputs": [],
   "source": [
    "def mse_loss(t, y):\n",
    "    return (t - y) ** 2\n",
    "\n",
    "def d_mse_loss(t, y):\n",
    "    return 2 * (y - t)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, n_inp, n_out, activation='sigmoid'):\n",
    "        self.w = torch.rand(n_out, n_inp) * 0.1\n",
    "        self.b = torch.rand(n_out, 1) * 0.1\n",
    "        #print(n_inp, n_out, activation)\n",
    "        if activation == 'sigmoid':\n",
    "            self.activ = sigmoid\n",
    "        elif activation == 'relu':\n",
    "            self.activ = relu\n",
    "        elif activation == 'None':\n",
    "            self.activ = None\n",
    "        else:\n",
    "            raise Exception(f'Unknown activation \"{activation}\"')\n",
    "        self._clear_state()\n",
    "\n",
    "    def _clear_state(self):\n",
    "        self.lin = None\n",
    "        self.inp = None\n",
    "        self.d_w = None\n",
    "        self.d_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.inp = x\n",
    "        self.lin = self.w @ x + self.b\n",
    "        activ = self.activ(self.lin) if self.activ is not None else self.lin\n",
    "\n",
    "        return activ\n",
    "\n",
    "    def backward(self, grad): # grad = d L / d z    Dout\n",
    "        # grad * dz / d lin\n",
    "        if self.activ == sigmoid:\n",
    "            grad_lin = sigmoid_backward(grad, self.lin)\n",
    "        elif self.activ == relu:\n",
    "            grad_lin = relu_backward(grad, self.lin)\n",
    "        else:\n",
    "            grad_lin = grad\n",
    "        # grad_lin * d lin / d w\n",
    "        m = self.inp.shape[1]\n",
    "        self.d_w = grad_lin @ self.inp.t() / m    # d_in dOut\n",
    "        # grad_lin * d lin / d b\n",
    "        self.d_b = torch.sum(grad_lin, axis=1, keepdims=True) / m\n",
    "        grad = self.w.t() @ grad_lin\n",
    "\n",
    "        return grad\n",
    "\n",
    "# pred = model(x)\n",
    "# loss = criterion(pred, target)\n",
    "# grad = d loss / d pred\n",
    "# model.backward(grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, arch: Tuple[Tuple[int, int]], activation):\n",
    "        self.layers = []\n",
    "        for i, p in enumerate(arch):\n",
    "            #print(i, p, activation)\n",
    "            self.layers.append(\n",
    "                LinearLayer(p[0], p[1], activation=activation if i < len(arch)-1 else 'None')\n",
    "            )\n",
    "        self._clear_state()\n",
    "\n",
    "    def _clear_state(self):\n",
    "        for l in self.layers:\n",
    "            l._clear_state()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "        return grad"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2: Написать 1-2 адаптивных оптимизатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "# SGD Momentum optimizer для всей модели\n",
    "# velocity = momentum * velocity - lr * gradient\n",
    "# w = w + velocity\n",
    "class SGDMomentum:\n",
    "    def __init__(self, model: Model, lr= 0.0001, momentum=0.99):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.m = momentum\n",
    "        self.vel = [[torch.zeros_like(layer.w),\n",
    "                     torch.zeros_like(layer.b)] for layer in model.layers]\n",
    "\n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            self.vel[i][0] = self.vel[i][0] * self.m - self.lr * layer.d_w\n",
    "            self.vel[i][1] = self.vel[i][1] * self.m - self.lr * layer.d_b\n",
    "            layer.w += self.vel[i][0]\n",
    "            layer.b += self.vel[i][1]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "# Adagrad optimizer для всей модели\n",
    "# accumulated += gradient ** 2\n",
    "# adapt_lr = lr/sqrt(accumulated)\n",
    "# w = w - adapt_lr * gradient\n",
    "\n",
    "class Adagrad:\n",
    "    def __init__(self, model: Model, lr= 0.0001):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.acc = [[torch.zeros_like(layer.w),\n",
    "                     torch.zeros_like(layer.b)] for layer in model.layers]\n",
    "\n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            self.acc[i][0] += layer.d_w ** 2\n",
    "            self.acc[i][1] += layer.d_b ** 2\n",
    "            adapt_lr_w = self.lr / torch.sqrt(self.acc[i][0])\n",
    "            adapt_lr_b = self.lr / torch.sqrt(self.acc[i][1])\n",
    "            layer.w -= adapt_lr_w * layer.d_w\n",
    "            layer.b -= adapt_lr_b * layer.d_b\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "# RMSProp optimizer для всей модели\n",
    "# accumulated = rho * accumulated + (1 - rho) * gradient ** 2\n",
    "# adapt_lr = lr/sqrt(accumulated)\n",
    "# w = w - adapt_lr * gradient\n",
    "\n",
    "class RMSProp:\n",
    "    def __init__(self, model: Model, lr= 0.0001, rho= 0.99):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.rho = rho\n",
    "        self.acc = [[torch.zeros_like(layer.w),\n",
    "                     torch.zeros_like(layer.b)] for layer in model.layers]\n",
    "\n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            self.acc[i][0] = self.rho * self.acc[i][0] + (1 - self.rho) * layer.d_w ** 2\n",
    "            self.acc[i][1] = self.rho * self.acc[i][1] + (1 - self.rho) * layer.d_b ** 2\n",
    "            adapt_lr_w = self.lr / torch.sqrt(self.acc[i][0])\n",
    "            adapt_lr_b = self.lr / torch.sqrt(self.acc[i][1])\n",
    "            layer.w -= adapt_lr_w * layer.d_w\n",
    "            layer.b -= adapt_lr_b * layer.d_b\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "# Adam optimizer для всей модели\n",
    "# velocity = beta1 * velocity + (1 - beta1) * gradient\n",
    "# accumulated = beta2 * accumulated + (1 - beta2) * gradient ** 2\n",
    "# adapt_lr = lr/sqrt(accumulated)\n",
    "# w = w - adapt_lr * velocity\n",
    "\n",
    "class Adam:\n",
    "    def __init__(self, model: Model, lr= 0.0001, beta1 = 0.99, beta2 = 0.99):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.acc = [[torch.zeros_like(layer.w),\n",
    "                     torch.zeros_like(layer.b)] for layer in model.layers]\n",
    "        self.vel = [[torch.zeros_like(layer.w),\n",
    "                     torch.zeros_like(layer.b)] for layer in model.layers]\n",
    "\n",
    "    def step(self):\n",
    "        for i, layer in enumerate(self.model.layers):\n",
    "            self.vel[i][0] = self.vel[i][0] * self.beta1 + (1 - self.beta1) * layer.d_w\n",
    "            self.vel[i][1] = self.vel[i][1] * self.beta1 + (1 - self.beta1) * layer.d_b\n",
    "            self.acc[i][0] = self.beta2 * self.acc[i][0] + (1 - self.beta2) * layer.d_w ** 2\n",
    "            self.acc[i][1] = self.beta2 * self.acc[i][1] + (1 - self.beta2) * layer.d_b ** 2\n",
    "            adapt_lr_w = self.lr / torch.sqrt(self.acc[i][0])\n",
    "            adapt_lr_b = self.lr / torch.sqrt(self.acc[i][1])\n",
    "            layer.w -= adapt_lr_w * self.vel[i][0]\n",
    "            layer.b -= adapt_lr_b * self.vel[i][1]\n",
    "\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.model._clear_state()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "# pred = model(x)\n",
    "# loss = criterion(pred, target)\n",
    "# grad = d loss / d pred\n",
    "# model.backward(grad)\n",
    "# optim.step()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "x = (torch.rand(2000)-0.5)*4\n",
    "y = x**2 + torch.randn(1)*0.1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[0.7576]]) tensor([[3.4543]]) tensor([[1.0391]]) tensor([[3.1115]])\n",
      "1 tensor([[0.9312]]) tensor([[3.7176]]) tensor([[0.9092]]) tensor([[3.6250]])\n",
      "2 tensor([[0.9225]]) tensor([[3.8022]]) tensor([[0.9177]]) tensor([[3.7616]])\n",
      "3 tensor([[0.9215]]) tensor([[3.8281]]) tensor([[0.9252]]) tensor([[3.8132]])\n",
      "4 tensor([[0.9229]]) tensor([[3.8399]]) tensor([[0.9285]]) tensor([[3.8345]])\n",
      "5 tensor([[0.9246]]) tensor([[3.8475]]) tensor([[0.9303]]) tensor([[3.8465]])\n",
      "6 tensor([[0.9261]]) tensor([[3.8532]]) tensor([[0.9315]]) tensor([[3.8551]])\n",
      "7 tensor([[0.9274]]) tensor([[3.8579]]) tensor([[0.9323]]) tensor([[3.8620]])\n",
      "8 tensor([[0.9285]]) tensor([[3.8620]]) tensor([[0.9330]]) tensor([[3.8679]])\n",
      "9 tensor([[0.9294]]) tensor([[3.8655]]) tensor([[0.9336]]) tensor([[3.8729]])\n",
      "10 tensor([[0.9302]]) tensor([[3.8686]]) tensor([[0.9341]]) tensor([[3.8774]])\n",
      "11 tensor([[0.9309]]) tensor([[3.8714]]) tensor([[0.9344]]) tensor([[3.8813]])\n",
      "12 tensor([[0.9316]]) tensor([[3.8739]]) tensor([[0.9348]]) tensor([[3.8849]])\n",
      "13 tensor([[0.9321]]) tensor([[3.8762]]) tensor([[0.9350]]) tensor([[3.8880]])\n",
      "14 tensor([[0.9326]]) tensor([[3.8784]]) tensor([[0.9353]]) tensor([[3.8909]])\n",
      "15 tensor([[0.9331]]) tensor([[3.8803]]) tensor([[0.9355]]) tensor([[3.8935]])\n",
      "16 tensor([[0.9335]]) tensor([[3.8821]]) tensor([[0.9357]]) tensor([[3.8958]])\n",
      "17 tensor([[0.9339]]) tensor([[3.8838]]) tensor([[0.9359]]) tensor([[3.8980]])\n",
      "18 tensor([[0.9343]]) tensor([[3.8854]]) tensor([[0.9361]]) tensor([[3.8999]])\n",
      "19 tensor([[0.9346]]) tensor([[3.8869]]) tensor([[0.9362]]) tensor([[3.9017]])\n"
     ]
    }
   ],
   "source": [
    "model1 = Model(((1, 100), (100, 1)), activation='sigmoid')\n",
    "optim_sgd = SGDMomentum(model1, lr=0.001)\n",
    "for e in range(20):\n",
    "    for i, (val, t) in enumerate(zip(x, y)):\n",
    "        optim_sgd.zero_grad()\n",
    "        pred = model1.forward(torch.tensor([[val]]))\n",
    "        loss = mse_loss(t, pred)\n",
    "        grad = d_mse_loss(t, pred)\n",
    "        model1.backward(grad)\n",
    "        optim_sgd.step()\n",
    "\n",
    "    print(e, model1.forward(torch.tensor([[1.]])), model1.forward(torch.tensor([[2.]])), model1.forward(torch.tensor([[-1.]])), model1.forward(torch.tensor([[-2.]])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[1.3472]]) tensor([[1.3716]]) tensor([[1.2982]]) tensor([[1.2737]])\n",
      "1 tensor([[1.3097]]) tensor([[1.3326]]) tensor([[1.2638]]) tensor([[1.2409]])\n",
      "2 tensor([[1.3005]]) tensor([[1.3226]]) tensor([[1.2562]]) tensor([[1.2340]])\n",
      "3 tensor([[1.2966]]) tensor([[1.3181]]) tensor([[1.2534]]) tensor([[1.2318]])\n",
      "4 tensor([[1.2943]]) tensor([[1.3154]]) tensor([[1.2520]]) tensor([[1.2309]])\n",
      "5 tensor([[1.2927]]) tensor([[1.3134]]) tensor([[1.2512]]) tensor([[1.2305]])\n",
      "6 tensor([[1.2915]]) tensor([[1.3119]]) tensor([[1.2507]]) tensor([[1.2303]])\n",
      "7 tensor([[1.2905]]) tensor([[1.3106]]) tensor([[1.2504]]) tensor([[1.2303]])\n",
      "8 tensor([[1.2897]]) tensor([[1.3094]]) tensor([[1.2501]]) tensor([[1.2304]])\n",
      "9 tensor([[1.2890]]) tensor([[1.3084]]) tensor([[1.2500]]) tensor([[1.2305]])\n",
      "10 tensor([[1.2883]]) tensor([[1.3075]]) tensor([[1.2499]]) tensor([[1.2306]])\n",
      "11 tensor([[1.2878]]) tensor([[1.3067]]) tensor([[1.2498]]) tensor([[1.2308]])\n",
      "12 tensor([[1.2872]]) tensor([[1.3059]]) tensor([[1.2497]]) tensor([[1.2310]])\n",
      "13 tensor([[1.2867]]) tensor([[1.3052]]) tensor([[1.2497]]) tensor([[1.2312]])\n",
      "14 tensor([[1.2863]]) tensor([[1.3046]]) tensor([[1.2497]]) tensor([[1.2314]])\n",
      "15 tensor([[1.2859]]) tensor([[1.3039]]) tensor([[1.2497]]) tensor([[1.2316]])\n",
      "16 tensor([[1.2855]]) tensor([[1.3034]]) tensor([[1.2497]]) tensor([[1.2318]])\n",
      "17 tensor([[1.2851]]) tensor([[1.3028]]) tensor([[1.2497]]) tensor([[1.2320]])\n",
      "18 tensor([[1.2848]]) tensor([[1.3023]]) tensor([[1.2497]]) tensor([[1.2322]])\n",
      "19 tensor([[1.2845]]) tensor([[1.3018]]) tensor([[1.2498]]) tensor([[1.2325]])\n"
     ]
    }
   ],
   "source": [
    "model2 = Model(((1, 100), (100, 1)), activation='sigmoid')\n",
    "optim_ada = Adagrad (model2, lr=0.001)\n",
    "for e in range(20):\n",
    "    for i, (val, t) in enumerate(zip(x, y)):\n",
    "        optim_ada.zero_grad()\n",
    "        pred = model2.forward(torch.tensor([[val]]))\n",
    "        loss = mse_loss(t, pred)\n",
    "        grad = d_mse_loss(t, pred)\n",
    "        model2.backward(grad)\n",
    "        optim_ada.step()\n",
    "\n",
    "    print(e, model2.forward(torch.tensor([[1.]])), model2.forward(torch.tensor([[2.]])), model2.forward(torch.tensor([[-1.]])), model2.forward(torch.tensor([[-2.]])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[1.1976]]) tensor([[1.2030]]) tensor([[1.1868]]) tensor([[1.1814]])\n",
      "1 tensor([[1.1923]]) tensor([[1.1889]]) tensor([[1.1993]]) tensor([[1.2028]])\n",
      "2 tensor([[1.1945]]) tensor([[1.1885]]) tensor([[1.2066]]) tensor([[1.2125]])\n",
      "3 tensor([[1.1986]]) tensor([[1.1918]]) tensor([[1.2125]]) tensor([[1.2195]])\n",
      "4 tensor([[1.2023]]) tensor([[1.1945]]) tensor([[1.2189]]) tensor([[1.2276]])\n",
      "5 tensor([[1.2001]]) tensor([[1.1887]]) tensor([[1.2293]]) tensor([[1.2465]])\n",
      "6 tensor([[1.1692]]) tensor([[1.1534]]) tensor([[1.2552]]) tensor([[1.3177]])\n",
      "7 tensor([[1.0922]]) tensor([[1.1112]]) tensor([[1.3035]]) tensor([[1.4962]])\n",
      "8 tensor([[1.0605]]) tensor([[1.2587]]) tensor([[1.3149]]) tensor([[1.7102]])\n",
      "9 tensor([[1.1019]]) tensor([[1.7249]]) tensor([[1.2392]]) tensor([[2.0335]])\n",
      "10 tensor([[1.1591]]) tensor([[2.4716]]) tensor([[1.1000]]) tensor([[2.6737]])\n",
      "11 tensor([[1.1823]]) tensor([[3.0212]]) tensor([[1.0002]]) tensor([[3.2392]])\n",
      "12 tensor([[1.1593]]) tensor([[3.1362]]) tensor([[0.9631]]) tensor([[3.3953]])\n",
      "13 tensor([[1.1324]]) tensor([[3.2067]]) tensor([[0.9514]]) tensor([[3.4703]])\n",
      "14 tensor([[1.1059]]) tensor([[3.2726]]) tensor([[0.9516]]) tensor([[3.5210]])\n",
      "15 tensor([[1.0801]]) tensor([[3.3360]]) tensor([[0.9580]]) tensor([[3.5593]])\n",
      "16 tensor([[1.0551]]) tensor([[3.3985]]) tensor([[0.9677]]) tensor([[3.5914]])\n",
      "17 tensor([[1.0314]]) tensor([[3.4617]]) tensor([[0.9782]]) tensor([[3.6221]])\n",
      "18 tensor([[1.0096]]) tensor([[3.5270]]) tensor([[0.9865]]) tensor([[3.6567]])\n",
      "19 tensor([[0.9909]]) tensor([[3.5952]]) tensor([[0.9893]]) tensor([[3.7001]])\n"
     ]
    }
   ],
   "source": [
    "model3 = Model(((1, 100), (100, 1)), activation='sigmoid')\n",
    "optim_rms = RMSProp(model3, lr=0.001)\n",
    "for e in range(20):\n",
    "    for i, (val, t) in enumerate(zip(x, y)):\n",
    "        optim_rms.zero_grad()\n",
    "        pred = model3.forward(torch.tensor([[val]]))\n",
    "        loss = mse_loss(t, pred)\n",
    "        grad = d_mse_loss(t, pred)\n",
    "        model3.backward(grad)\n",
    "        optim_rms.step()\n",
    "\n",
    "    print(e, model3.forward(torch.tensor([[1.]])), model3.forward(torch.tensor([[2.]])), model3.forward(torch.tensor([[-1.]])), model3.forward(torch.tensor([[-2.]])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[1.2384]]) tensor([[1.2441]]) tensor([[1.2270]]) tensor([[1.2213]])\n",
      "1 tensor([[1.2263]]) tensor([[1.2230]]) tensor([[1.2328]]) tensor([[1.2361]])\n",
      "2 tensor([[1.2219]]) tensor([[1.2163]]) tensor([[1.2331]]) tensor([[1.2386]])\n",
      "3 tensor([[1.2200]]) tensor([[1.2142]]) tensor([[1.2319]]) tensor([[1.2379]])\n",
      "4 tensor([[1.2205]]) tensor([[1.2165]]) tensor([[1.2295]]) tensor([[1.2345]])\n",
      "5 tensor([[1.2309]]) tensor([[1.2411]]) tensor([[1.2201]]) tensor([[1.2208]])\n",
      "6 tensor([[1.2712]]) tensor([[1.3424]]) tensor([[1.1875]]) tensor([[1.1898]])\n",
      "7 tensor([[1.3294]]) tensor([[1.5286]]) tensor([[1.1475]]) tensor([[1.2234]])\n",
      "8 tensor([[1.3336]]) tensor([[1.7530]]) tensor([[1.1644]]) tensor([[1.4988]])\n",
      "9 tensor([[1.2958]]) tensor([[2.2393]]) tensor([[1.2439]]) tensor([[2.1038]])\n",
      "10 tensor([[1.2091]]) tensor([[3.0710]]) tensor([[1.3141]]) tensor([[2.9273]])\n",
      "11 tensor([[1.0799]]) tensor([[3.4133]]) tensor([[1.2526]]) tensor([[3.1776]])\n",
      "12 tensor([[1.0373]]) tensor([[3.4970]]) tensor([[1.2309]]) tensor([[3.2647]])\n",
      "13 tensor([[1.0206]]) tensor([[3.5453]]) tensor([[1.2139]]) tensor([[3.3485]])\n",
      "14 tensor([[1.0132]]) tensor([[3.5766]]) tensor([[1.1968]]) tensor([[3.4261]])\n",
      "15 tensor([[1.0086]]) tensor([[3.5976]]) tensor([[1.1765]]) tensor([[3.4931]])\n",
      "16 tensor([[1.0045]]) tensor([[3.6121]]) tensor([[1.1507]]) tensor([[3.5451]])\n",
      "17 tensor([[0.9849]]) tensor([[3.6064]]) tensor([[1.1002]]) tensor([[3.5556]])\n",
      "18 tensor([[0.9368]]) tensor([[3.5674]]) tensor([[1.0250]]) tensor([[3.5261]])\n",
      "19 tensor([[0.9243]]) tensor([[3.5691]]) tensor([[1.0159]]) tensor([[3.5840]])\n"
     ]
    }
   ],
   "source": [
    "model4 = Model(((1, 100), (100, 1)), activation='sigmoid')\n",
    "optim_adam = Adam(model4, lr=0.001)\n",
    "for e in range(20):\n",
    "    for i, (val, t) in enumerate(zip(x, y)):\n",
    "        optim_adam.zero_grad()\n",
    "        pred = model4.forward(torch.tensor([[val]]))\n",
    "        loss = mse_loss(t, pred)\n",
    "        grad = d_mse_loss(t, pred)\n",
    "        model4.backward(grad)\n",
    "        optim_adam.step()\n",
    "\n",
    "    print(e, model4.forward(torch.tensor([[1.]])), model4.forward(torch.tensor([[2.]])), model4.forward(torch.tensor([[-1.]])), model4.forward(torch.tensor([[-2.]])))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3: Решить задачу нахождения корней квадратного уравнения методом градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Task 1\n",
    "# Find the roots of square equation by gradient descent\n",
    "# x ** 2 - 6 * x + 4 = 0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# возвести в квадрат\n",
    "# посчитать производную\n",
    "# надо начать движение от начальной точки в направлении антградиента с заданным шагом\n",
    "# x = x - lr * grad(x)\n",
    "# всегда ли сойдемся за приемлемое количество шагов?\n",
    "# важна ли начальная точка?\n",
    "# как найти второй корень?\n",
    "# как вляет ЛР?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 0.7641048017066163\n"
     ]
    }
   ],
   "source": [
    "# ax2 + bx + c = 0\n",
    "a = 1\n",
    "b = -6\n",
    "c = 4\n",
    "\n",
    "# ax2 + bx + c = 0\n",
    "# производная ф-ции 2ax + b\n",
    "# То минимум ф-ции ax2 + bx + c находится в точке x = -b/2a. Тогда двигаясь вправо или влево можно найти 2 корня уравнения.\n",
    "\n",
    "# для нахождения корней возведем ф-цию в квадрат\n",
    "# (ax2 + bx + c)2 = 0\n",
    "# a2x4 + b2x2 + c2 + 2abx3 + 2acx2 + 2bcx = 0\n",
    "# найдем производную\n",
    "# 4a2x3 + 2b2x + 6abx2 + 4acx + 2bc = 0\n",
    "# 4a2x3 + 6abx2 + (2b2+4ac)x + 2bc = 0\n",
    "\n",
    "def f(x):\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "def grad(x):\n",
    "    return 4 * a**2 * x**3 + 6 * a * b * x**2 + (2 * b**2 + 4 * a * c) * x + 2 * b * c\n",
    "\n",
    "centr = -b / 2 / a\n",
    "lr = 0.01\n",
    "x1 = centr - lr\n",
    "accuracy = 0.0001\n",
    "e = 0\n",
    "x = x1 - lr * grad(x1)\n",
    "while abs(x - x1) > accuracy:\n",
    "    x1 = x\n",
    "    #print(e, x1)\n",
    "    e += 1\n",
    "    x = x1 - lr * grad(x1)\n",
    "print(e, x1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 5.235895198293384\n"
     ]
    }
   ],
   "source": [
    "x2 = centr + lr\n",
    "e = 0\n",
    "x = x2 - lr * grad(x2)\n",
    "while abs(x - x2) > accuracy:\n",
    "    x2 = x\n",
    "    #print(e, x2)\n",
    "    e += 1\n",
    "    x = x2 - lr * grad(x2)\n",
    "print(e, x2)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# всегда ли сойдемся за приемлемое количество шагов?\n",
    "# Ответ: зависит от LR и заданной точности\n",
    "\n",
    "# важна ли начальная точка?\n",
    "# Ответ: для нахождения обоих корней начальная точка важна, от этого зависит в какой минимум будем спускаться\n",
    "\n",
    "\n",
    "# как найти второй корень?\n",
    "# Ответ: тк ф-ция симметрична, найдем центр и будем спускаться вправо и влево\n",
    "\n",
    "# как вляет ЛР?\n",
    "# Ответ: влияет на скорость спуска\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
